from pathlib import Path
import pdfplumber
import chromadb
from openai import OpenAI
import json
import re
import os
from dotenv import load_dotenv

# ---------------------- Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¸ ----------------------


def extract_pdf_text(pdf_path: str) -> str:
    path = Path(pdf_path)
    if not path.exists():
        raise FileNotFoundError(f"Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {path.resolve()}")

    pages_text = []
    with pdfplumber.open(path) as pdf:
        for idx, page in enumerate(pdf.pages, start=1):
            text = page.extract_text() or ""
            text = text.replace("\xa0", " ").strip()
            pages_text.append(
                f"\n\n=== PAGE {idx}/{len(pdf.pages)} ===\n{text}")
    return "\n".join(pages_text).strip()


def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50):
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + chunk_size
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks


def save_text(text: str, out_path: Path):
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(text, encoding="utf-8")
    print(f"Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½ txt: {out_path}")

# ---------------------- ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² ----------------------


load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=api_key)

chroma_client = chromadb.PersistentClient(path="chroma_db")
collection = chroma_client.get_or_create_collection("all_cvs")

# ---------------------- Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ PDF ----------------------

PDF_FOLDER = Path("pdfs")        # Ð¿Ð°Ð¿ÐºÐ° Ñ PDF
TXT_FOLDER = Path("pdf_txts")    # Ð¿Ð°Ð¿ÐºÐ° Ð´Ð»Ñ txt

# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… id Ð² ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ (Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸)
existing_ids = set(collection.get()["ids"])

for pdf_file in PDF_FOLDER.glob("*.pdf"):
    pdf_id_prefix = pdf_file.stem

    # Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÐµÑÑ‚ÑŒ Ð»Ð¸ ÑƒÐ¶Ðµ ÑÑ‚Ð¾Ñ‚ PDF Ð² ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
    already_indexed = any(id_.startswith(pdf_id_prefix)
                          for id_ in existing_ids)
    if already_indexed:
        print(f"âš¡ PDF {pdf_file.name} ÑƒÐ¶Ðµ Ð² Ð±Ð°Ð·Ðµ, Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼.")
        continue

    else:
        print(f"ðŸ“„ ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ PDF: {pdf_file.name}...")
        text = extract_pdf_text(pdf_file)

        # ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ txt
        txt_file = TXT_FOLDER / f"{pdf_file.stem}.txt"
        save_text(text, txt_file)

    # ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ embeddings
    chunks = chunk_text(text)
    for i, chunk in enumerate(chunks):
        embedding = client.embeddings.create(
            input=chunk,
            model="text-embedding-3-small"
        ).data[0].embedding
        collection.add(
            ids=[f"{pdf_file.stem}_chunk_{i}"],
            embeddings=[embedding],
            documents=[chunk],
            metadatas=[{"pdf_name": pdf_file.name, "chunk_index": i}]
        )

print("âœ… Ð’ÑÐµ PDF Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ñ‹ Ð¸ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð² ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ.")


# ---------------------- Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° ----------------------

chat_history = []  # Ð·Ð´ÐµÑÑŒ Ð±ÑƒÐ´ÐµÐ¼ Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ (Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹)


def rerank_and_answer(question: str,
                      candidates: int = 10,
                      top_k: int = 3,
                      reranker_model: str = "gpt-3.5-turbo",
                      answer_model: str = "gpt-4o-mini"):
    """
    1) Ð‘ÐµÑ€Ñ‘Ð¼ `candidates` Ð¸Ð· Chroma (Ð¿Ð¾ embedding Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°).
    2) ÐŸÑ€Ð¾ÑÐ¸Ð¼ LLM (reranker_model) Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ top_k Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð² (0-based) Ð² Ð²Ð¸Ð´Ðµ JSON ÑÐ¿Ð¸ÑÐºÐ°.
    3) Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ context Ð¸Ð· Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² Ð¸ Ð·Ð°Ð¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÐ¼ Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ñƒ answer_model.
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ (answer, debug) Ð³Ð´Ðµ debug ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ candidates, reranker_raw, selected_indices, final_context.
    """

    # 1. ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° Ð¸ Ð¿Ð¾Ð¸ÑÐº ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð²
    qemb = client.embeddings.create(
        model="text-embedding-3-small", input=question)
    qvec = qemb.data[0].embedding

    results = collection.query(
        query_embeddings=[qvec],
        n_results=candidates,
        include=["documents", "metadatas", "distances"]
    )

    cand_texts = results.get("documents", [[]])[0]
    cand_metas = results.get("metadatas",  [[]])[0]
    cand_dists = results.get("distances",  [[]])[0]

    # Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ: ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² â€” ÑÑ€Ð°Ð·Ñƒ fallback
    if not cand_texts:
        return "ÐÐ¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð² Ð±Ð°Ð·Ðµ.", {"error": "no_candidates"}

    # 2. Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ñ€ÐµÑ„Ð¸ÐºÑ Ñ Ð½ÑƒÐ¼ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹ â€” ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ðµ Ð¿Ñ€ÐµÐ²ÑŒÑŽ Ð´Ð»Ñ reranker'Ð° (Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿Ñ€Ð¸ÑÑ‹Ð»Ð°Ñ‚ÑŒ Ð¾Ð³Ñ€Ð¾Ð¼Ð½Ñ‹Ðµ Ñ‚ÐµÐºÑÑ‚Ñ‹)
    #    ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 800 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð° (Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ€ÐµÑ€Ð°Ð½ÐºÐµÑ€ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð» ÑÑƒÑ‚ÑŒ).
    preview_max = 800
    enumerated = []
    for i, txt in enumerate(cand_texts):
        preview = txt if len(txt) <= preview_max else txt[:preview_max].rsplit(" ", 1)[
            0] + "..."
        meta = cand_metas[i] if i < len(cand_metas) else {}
        source = meta.get("pdf_name", "unknown")
        enumerated.append(f"{i}: [{source}] {preview}")

    candidates_block = "\n".join(enumerated)

    # 3. ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ñ€ÐµÑ€Ð°Ð½Ð³ÐµÑ€Ð° â€” Ð¿Ñ€Ð¾ÑÐ¸Ð¼ Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ JSON ÑÐ¿Ð¸ÑÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð² (0-based)
    rerank_prompt = f"""
Ð£ Ñ‚ÐµÐ±Ñ ÐµÑÑ‚ÑŒ Ð’ÐžÐŸÐ ÐžÐ¡:
{question}

ÐÐ¸Ð¶Ðµ ÑÐ¿Ð¸ÑÐ¾Ðº ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² (Ñ€Ð°Ð·Ð¼ÐµÑ‡ÐµÐ½ Ð¸Ð½Ð´ÐµÐºÑÐ°Ð¼Ð¸ ÑÐ»ÐµÐ²Ð°). ÐšÐ°Ð¶Ð´Ñ‹Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ â€” Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ (preview) Ñ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð¼.
Ð—Ð°Ð´Ð°Ñ‡Ð°: Ð²Ñ‹Ð±ÐµÑ€Ð¸ {top_k} Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¿Ð¾Ð¼Ð¾Ð³ÑƒÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ.
ÐžÐ¢Ð’Ð•Ð§ÐÐ™ Ð¢ÐžÐ›Ð¬ÐšÐž JSON ÐœÐÐ¡Ð¡Ð˜Ð’ÐžÐœ Ð˜ÐÐ”Ð•ÐšÐ¡ÐžÐ’ (0-based), Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: [0, 3, 7]
ÐÐ¸ Ð² ÐºÐ¾ÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ðµ Ð½Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸Ð¹ â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ€Ð°Ð±Ð¾Ñ‡Ð¸Ð¹ JSON.

ÐšÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ñ‹:
{candidates_block}
"""

    # 4. Ð—Ð°Ð¿Ñ€Ð¾Ñ Ðº reranker (Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ â€” Ð´ÐµÑˆÐµÐ²Ð»Ðµ)
    rerank_resp = client.chat.completions.create(
        model=reranker_model,
        messages=[
            {"role": "system", "content": "Ð¢Ñ‹ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑˆÑŒ Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹ Ñ‚ÐµÐºÑÑ‚Ð°."},
            {"role": "user", "content": rerank_prompt}
        ],
        temperature=0.0,  # Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ Ð»ÑƒÑ‡ÑˆÐµ Ð´Ð»Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
    )
    rerank_raw = rerank_resp.choices[0].message.content.strip()

    # 5. ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ JSON Ð¸Ð· Ð¾Ñ‚Ð²ÐµÑ‚Ð°
    selected_indices = None
    try:
        # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð½Ð°Ð¹Ñ‚Ð¸ JSON array Ð² Ñ‚ÐµÐºÑÑ‚Ðµ
        m = re.search(r"(\[.*\])", rerank_raw, re.DOTALL)
        if m:
            candidate_json = m.group(1)
            selected_indices = json.loads(candidate_json)
            # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ (ÑƒÐ±ÐµÐ´Ð¸Ð¼ÑÑ, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ ÑÐ¿Ð¸ÑÐ¾Ðº int)
            selected_indices = [int(x) for x in selected_indices][:top_k]
    except Exception:
        selected_indices = None

    # 6. Ð¤Ð¾Ð»Ð»Ð±ÐµÐº: ÐµÑÐ»Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð½Ðµ ÑƒÐ´Ð°Ð»ÑÑ â€” Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ñ (Ð¼ÐµÐ½ÑŒÑˆÐµ distance == Ð±Ð»Ð¸Ð¶Ðµ)
    if not selected_indices:
        # ÑÐ¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ distance Ð¸ Ð±ÐµÑ€Ñ‘Ð¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ top_k Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð²
        try:
            idxs_sorted = sorted(range(len(cand_dists)),
                                 key=lambda i: cand_dists[i])
            selected_indices = idxs_sorted[:top_k]
        except Exception:
            # ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ distances â€” Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð±ÐµÑ€ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ top_k
            selected_indices = list(range(min(top_k, len(cand_texts))))

    # 7. Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¸Ð· Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… chunks (Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…)
    final_parts = []
    for i in selected_indices:
        meta = cand_metas[i] if i < len(cand_metas) else {}
        source = meta.get("pdf_name", "unknown")
        chunk_idx = meta.get("chunk_index", "?")
        final_parts.append(f"[{source} | chunk {chunk_idx}]\n{cand_texts[i]}")

    final_context = "\n\n---\n\n".join(final_parts)

    # 8. Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº LLM â€” Ð¾Ñ‚Ð²ÐµÑ‚ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ð¾Ñ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚
    system_msg = {
        "role": "system",
        "content": "Ð¢Ñ‹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°. Ð•ÑÐ»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ â€” Ñ‡ÐµÑÑ‚Ð½Ð¾ ÑÐºÐ°Ð¶Ð¸."
    }
    user_msg = {
        "role": "user",
        "content": f"ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:\n{final_context}\n\nÐ’Ð¾Ð¿Ñ€Ð¾Ñ: {question}"
    }

    answer_resp = client.chat.completions.create(
        model=answer_model,
        messages=[system_msg, user_msg],
        temperature=0.0
    )
    answer = answer_resp.choices[0].message.content

    debug = {
        "candidates_count": len(cand_texts),
        "candidates_preview": enumerated,
        "reranker_raw": rerank_raw,
        "selected_indices": selected_indices,
        "distances": cand_dists
    }

    return answer, debug

# ---------------------- Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼ ----------------------


if __name__ == "__main__":
    while True:
        user_input = input("Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð˜Ð˜ (Ð¸Ð»Ð¸ 'Ð²Ñ‹Ñ…Ð¾Ð´' Ð´Ð»Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ): ")
        if user_input.lower() == "Ð²Ñ‹Ñ…Ð¾Ð´":
            print("Ð’Ñ‹Ñ…Ð¾Ð´ Ð¸Ð· Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹.")
            break
        answer, debug = rerank_and_answer(user_input, candidates=10, top_k=3)
        print("\n=== DEBUG ===")
        print("Reranker raw:\n", debug["reranker_raw"])
        print("Selected indices:", debug["selected_indices"])
        print("\n=== ANSWER ===\n", answer)
